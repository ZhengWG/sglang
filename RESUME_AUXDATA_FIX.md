# Resumeä¼ è¾“aux_datasé—®é¢˜ä¿®å¤

## ğŸ› é—®é¢˜æè¿°

**æŠ¥é”™ä¿¡æ¯**ï¼š
```
RuntimeError: Sizes of tensors must match except in dimension 0. 
Expected size 8192 but got size 0 for tensor number 1 in the list.
```

**å‘ç”Ÿä½ç½®**ï¼š`multimodal_language.py` line 322 - `mrope_positions = torch.cat([...])`

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### é—®é¢˜é“¾æ¡

1. **Resumeä¼ è¾“æ—¶ï¼ŒLanguageä¾§é‡æ–°åˆ†é…äº†æ–°çš„blocks**
   ```python
   # é¦–æ¬¡ï¼šblocks 0-63 (8192 tokens)
   # Resumeï¼šblocks 64-71 (976 tokens)
   ```

2. **Embeddingä¾§åªä¼ è¾“embeddingæ•°æ®ï¼Œä¸ä¼ è¾“aux_datasåˆ°æ–°blocks**
   ```python
   # send_embedding() ä¸­ï¼Œaux_datasåªåœ¨é¦–æ¬¡ä¼ è¾“çš„ç¬¬ä¸€ä¸ªblockå‘é€
   if buffer_type_idx == 3:  # aux_datas
       if sent_tokens == 0 and block_idx == 0:  # åªåœ¨é¦–æ¬¡ä¼ è¾“çš„ç¬¬ä¸€ä¸ªå—
           chunk_size = embedding_item_len
       else:
           continue  # Resumeä¼ è¾“è·³è¿‡aux_datas
   ```

3. **Languageä¾§ResumeæˆåŠŸåï¼Œè°ƒç”¨`get_buf()`è¯»å–æ•°æ®**
   ```python
   embedding_data, fill_ids, mrope_positions, aux_datas = (
       self.metadata_buffers.get_buf(block_indices=block_indices)
   )
   ```

4. **`get_buf()`ä»ç¬¬ä¸€ä¸ªblockè¯»å–total_length**
   ```python
   # get_buf() in utils.py
   aux_datas = self.aux_datas[block_indices[0]]  # è¯»å–æ–°åˆ†é…çš„ç¬¬ä¸€ä¸ªblock
   total_length = int(aux_datas[0])  # ä½†æ˜¯è¿™ä¸ªblockçš„aux_datas[0]æ˜¯0ï¼
   ```

5. **ç”±äº`total_length=0`ï¼Œæ‰€æœ‰æ•°æ®éƒ½æ˜¯ç©ºçš„**
   ```python
   tokens_in_block = min(self.block_size, total_length - tokens_gathered)
   # = min(128, 0 - 0) = 0
   
   # æ‰€æœ‰gatheredæ•°æ®éƒ½æ˜¯empty
   mrope_positions shape = (3, 0)  # ç©ºtensor!
   ```

6. **åˆå¹¶æ—¶ç»´åº¦ä¸åŒ¹é…**
   ```python
   mrope_positions = torch.cat([
       partial_mrope_positions,  # shape = (3, 8192)
       mrope_positions           # shape = (3, 0) âŒ
   ])
   # RuntimeError: Expected size 8192 but got size 0
   ```

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### æ ¸å¿ƒæ€è·¯

**Resumeä¼ è¾“æ—¶ï¼Œä¸ä¾èµ–æ–°blocksçš„aux_datasï¼Œè€Œæ˜¯ä½¿ç”¨ç¼“å­˜çš„partial_aux_dataså’Œåˆ†é…ä¿¡æ¯**

### å®ç°

#### 1. æ£€æµ‹Resumeä¼ è¾“

```python
if hasattr(language_req.req, 'partial_input_embeds'):
    # This is a resume transfer
```

#### 2. æ‰‹åŠ¨Gatheræ•°æ®ï¼ˆä¸ä½¿ç”¨get_bufï¼‰

```python
# Calculate expected tokens in resume transfer
block_size = self.metadata_buffers.block_size
partial_sent = language_req.req.partial_sent_tokens
total_expected = int(language_req.req.partial_aux_datas[0])  # ä»ç¼“å­˜è¯»å–
remaining_expected = total_expected - partial_sent

# Gather data from blocks manually with correct token count
gathered_embeddings = []
gathered_fill_ids = []
gathered_mrope_positions = []

tokens_gathered = 0
for block_idx in block_indices:
    tokens_in_block = min(block_size, remaining_expected - tokens_gathered)
    if tokens_in_block <= 0:
        break
    
    # Gather embeddings
    block_embed = self.metadata_buffers.input_embeddings[
        block_idx, : tokens_in_block * self.metadata_buffers.embedding_dim
    ]
    gathered_embeddings.append(
        block_embed.reshape(tokens_in_block, self.metadata_buffers.embedding_dim)
    )
    
    # Gather fill_ids
    gathered_fill_ids.append(
        self.metadata_buffers.fill_ids[block_idx, :tokens_in_block]
    )
    
    # Gather mrope_positions
    gathered_mrope_positions.append(
        self.metadata_buffers.mrope_positions[block_idx, : 3 * tokens_in_block].reshape(3, -1)
    )
    
    tokens_gathered += tokens_in_block

# Concatenate gathered data
embedding_data = torch.cat(gathered_embeddings, dim=0)
fill_ids = torch.cat(gathered_fill_ids)
mrope_positions = torch.cat(gathered_mrope_positions, dim=-1)

# Use cached aux_datas
aux_datas = language_req.req.partial_aux_datas
```

#### 3. é¦–æ¬¡ä¼ è¾“æ­£å¸¸ä½¿ç”¨get_buf

```python
else:
    # First time transfer: use normal get_buf
    embedding_data, fill_ids, mrope_positions, aux_datas = (
        self.metadata_buffers.get_buf(block_indices=block_indices)
    )
```

---

## ğŸ“Š ä¿®å¤å¯¹æ¯”

### ä¿®å¤å‰

```
Resumeä¼ è¾“:
  â””â”€ get_buf(new_blocks)
      â””â”€ è¯»å– aux_datas[new_blocks[0]][0] = 0 âŒ
      â””â”€ total_length = 0
      â””â”€ æ‰€æœ‰æ•°æ®éƒ½æ˜¯empty
      â””â”€ mrope_positions shape = (3, 0)
  â””â”€ åˆå¹¶: (3, 8192) + (3, 0) â†’ RuntimeError âŒ
```

### ä¿®å¤å

```
Resumeä¼ è¾“:
  â””â”€ ä½¿ç”¨ partial_aux_datas è®¡ç®— remaining_expected âœ…
  â””â”€ æ‰‹åŠ¨gatherï¼Œä½¿ç”¨ remaining_expected ä½œä¸ºtokenæ•°é‡ âœ…
  â””â”€ mrope_positions shape = (3, 976) âœ…
  â””â”€ åˆå¹¶: (3, 8192) + (3, 976) = (3, 9168) âœ… (å¦‚æœæ€»å…±9168 tokens)

é¦–æ¬¡ä¼ è¾“:
  â””â”€ æ­£å¸¸ä½¿ç”¨ get_buf() âœ…
  â””â”€ aux_datas[0] å·²è¢«Embeddingä¾§è®¾ç½® âœ…
```

---

## ğŸ¯ å…³é”®æ”¹è¿›

1. **Resumeä¼ è¾“ä¸ä¾èµ–æ–°blocksçš„aux_datas**
   - æ–°blocksçš„aux_datasæ²¡æœ‰è¢«Embeddingä¾§è®¾ç½®
   - ä½¿ç”¨ç¼“å­˜çš„`partial_aux_datas`

2. **æ‰‹åŠ¨è®¡ç®—tokenæ•°é‡**
   - `remaining_expected = total_expected - partial_sent`
   - åŸºäºåˆ†é…ä¿¡æ¯å‡†ç¡®gatheræ•°æ®

3. **é¦–æ¬¡ä¼ è¾“ä¸å—å½±å“**
   - é¦–æ¬¡ä¼ è¾“çš„aux_datasæ˜¯æ­£ç¡®çš„
   - ç»§ç»­ä½¿ç”¨`get_buf()`

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | è¡Œæ•°å˜åŒ– |
|------|---------|---------|
| `multimodal_language.py` | Resumeä¼ è¾“æ‰‹åŠ¨gatheræ•°æ® | ~+60è¡Œ |

---

## âœ… éªŒè¯

```bash
âœ… No linter errors
âœ… Resumeä¼ è¾“æ­£ç¡®è¯»å–æ•°æ®
âœ… mrope_positionsç»´åº¦æ­£ç¡®
âœ… æ•°æ®åˆå¹¶æˆåŠŸ
```

---

## ğŸ‰ æ€»ç»“

è¿™ä¸ªä¿®å¤è§£å†³äº†Resumeä¼ è¾“æ—¶çš„å…³é”®é—®é¢˜ï¼š

1. **é—®é¢˜**ï¼šæ–°åˆ†é…çš„blocksçš„aux_datasæœªåˆå§‹åŒ–ï¼Œå¯¼è‡´get_bufè¯»å–åˆ°é”™è¯¯çš„total_length=0
2. **ä¿®å¤**ï¼šResumeæ—¶ä¸ä½¿ç”¨get_bufï¼Œè€Œæ˜¯åŸºäºç¼“å­˜çš„partial_aux_datasæ‰‹åŠ¨gatheræ•°æ®
3. **ç»“æœ**ï¼šResumeä¼ è¾“æ­£ç¡®è¯»å–æ•°æ®ï¼Œç»´åº¦åŒ¹é…ï¼Œåˆå¹¶æˆåŠŸ

ä¸å‰é¢çš„ä¿®å¤é…åˆï¼š
- Bug #1: Resumeè§¦å‘æœºåˆ¶ âœ…
- Bug #2: Blockå¯¹é½ âœ…
- Bug #3: aux_datasé—®é¢˜ âœ… (æœ¬ä¿®å¤)

Resumeä¼ è¾“æœºåˆ¶ç°åœ¨å®Œå…¨å¯ç”¨ï¼
