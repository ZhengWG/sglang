# å¤šTP RankåŒæ­¥é—®é¢˜ä¿®å¤

## ğŸ› é—®é¢˜æè¿°

**æŠ¥é”™æ—¥å¿—**ï¼š
```
[TP0] Resume transfer initiated: allocated 2 blocks (16384 tokens) âœ…
[TP2] Resume transfer initiated: allocated 2 blocks (16384 tokens) âœ…
[TP1] Unexpected: Transferring status but sent_tokens=0 >= actual_total_length=0 âŒ
[TP3] Unexpected: Transferring status but sent_tokens=0 >= actual_total_length=0 âŒ
```

**ç”¨æˆ·åé¦ˆ**ï¼šå› ä¸ºaux_dataåªå‘é€äº†ç¬¬ä¸€ä¸ªblock

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### é—®é¢˜é“¾æ¡

#### 1. aux_datasåªå†™å…¥Embeddingä¾§çš„ç¬¬ä¸€ä¸ªblock

Embeddingä¾§åœ¨`set_buf()`ä¸­ï¼š
```python
# Store metadata in first block
if block_idx_pos == 0:
    self.aux_datas[block_id][0] = embed_length  # åªå†™å…¥embedding_indices[0]
```

#### 2. ä¸åŒTP rankåˆ†é…ä¸åŒçš„blocks

```
å¤šTPåœºæ™¯ä¸‹çš„blockåˆ†é…ï¼š
TP0: embedding_indices = [0, 1, 2, ..., 7]   â†’ ç¬¬ä¸€ä¸ªblock = 0
TP1: embedding_indices = [8, 9, 10, ..., 15] â†’ ç¬¬ä¸€ä¸ªblock = 8
TP2: embedding_indices = [16, 17, 18, ..., 23] â†’ ç¬¬ä¸€ä¸ªblock = 16
TP3: embedding_indices = [24, 25, 26, ..., 31] â†’ ç¬¬ä¸€ä¸ªblock = 24
```

#### 3. Embeddingä¾§åªåœ¨block 0å†™å…¥aux_datas

```
Embeddingä¾§è°ƒç”¨ set_buf(req):
    req.embedding_indices = [0, 1, 2, ...]  # Embeddingä¾§çš„åˆ†é…
    aux_datas[0][0] = 2000  # åªå†™å…¥block 0
    
ç»“æœï¼š
    aux_datas[0] = [2000, ...]  âœ… TP0èƒ½è¯»åˆ°
    aux_datas[8] = [0, ...]     âŒ TP1è¯»åˆ°0
    aux_datas[16] = [0, ...]    âŒ TP2è¯»åˆ°0
    aux_datas[24] = [0, ...]    âŒ TP3è¯»åˆ°0
```

#### 4. Statusé€šè¿‡all_reduceåŒæ­¥ï¼Œæ‰€æœ‰rankéƒ½æ”¶åˆ°Transferring

```python
# poll_and_all_reduce() ç¡®ä¿æ‰€æœ‰rankå¾—åˆ°ç›¸åŒçš„status
æ‰€æœ‰rank: poll = KVPoll.Transferring
```

#### 5. å„rankè¯»å–è‡ªå·±çš„aux_datas

```python
# Languageä¾§å„rankè¯»å–
TP0: aux_datas = self.aux_datas[0]  â†’ [2000, ...] âœ…
TP1: aux_datas = self.aux_datas[8]  â†’ [0, ...] âŒ
TP2: aux_datas = self.aux_datas[16] â†’ [2000, ...] âœ… (å¯èƒ½æ”¶åˆ°äº†æ•°æ®)
TP3: aux_datas = self.aux_datas[24] â†’ [0, ...] âŒ
```

#### 6. TP1/TP3åˆ¤æ–­é”™è¯¯

```python
actual_total_length = int(aux_datas[0])  # = 0 âŒ
sent_tokens = len(fill_ids)              # = 0 âŒ

if actual_total_length > sent_tokens:    # 0 > 0 = False âŒ
    # ä¸è¿›å…¥resumeæµç¨‹
else:
    # è¾“å‡º: "Unexpected: sent_tokens=0 >= actual_total_length=0"
```

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### æ ¸å¿ƒæ€è·¯

**ä½¿ç”¨all_reduceåŒæ­¥aux_datasä¿¡æ¯ï¼Œç¡®ä¿æ‰€æœ‰rankè·å¾—ä¸€è‡´çš„actual_total_lengthå’Œsent_tokens**

### å®ç°

#### 1. åŒæ­¥actual_total_lengthå’Œsent_tokens

```python
elif poll == KVPoll.Transferring:
    # Get data from local buffer
    embedding_data, fill_ids, mrope_positions, aux_datas = (
        self.metadata_buffers.get_buf(block_indices=block_indices)
    )
    
    # Local values (may be 0 on some ranks)
    actual_total_length = int(aux_datas[0])
    sent_tokens = len(fill_ids)
    
    # Sync across all ranks using MAX (the rank with data has non-zero values)
    import torch.distributed as dist
    if self.gloo_group is not None:
        actual_total_length_tensor = torch.tensor([actual_total_length], dtype=torch.int64)
        sent_tokens_tensor = torch.tensor([sent_tokens], dtype=torch.int64)
        
        dist.all_reduce(actual_total_length_tensor, op=dist.ReduceOp.MAX, group=self.gloo_group)
        dist.all_reduce(sent_tokens_tensor, op=dist.ReduceOp.MAX, group=self.gloo_group)
        
        actual_total_length = int(actual_total_length_tensor.item())
        sent_tokens = int(sent_tokens_tensor.item())
    
    # Now all ranks have the same values âœ…
    if actual_total_length > sent_tokens:
        # Resume...
```

#### 2. åŒºåˆ†æœ‰æ•°æ®çš„rankå’Œdummy rank

```python
# Cache partial data
if not hasattr(language_req.req, 'partial_input_embeds'):
    has_data = (len(fill_ids) > 0)
    
    if has_data:
        # Real rank with data
        language_req.req.partial_input_embeds = embedding_data
        language_req.req.partial_fill_ids = fill_ids.tolist()
        language_req.req.partial_mrope_positions = mrope_positions
        language_req.req.partial_aux_datas = torch.tensor([actual_total_length, aux_datas[1]])
        language_req.req.partial_sent_tokens = sent_tokens
    else:
        # Dummy rank: create placeholder
        language_req.req.partial_input_embeds = torch.empty(0, embedding_dim)
        language_req.req.partial_fill_ids = []
        language_req.req.partial_mrope_positions = torch.empty(3, 0, dtype=torch.int32)
        language_req.req.partial_aux_datas = torch.tensor([actual_total_length, 0])
        language_req.req.partial_sent_tokens = sent_tokens
```

#### 3. æ‰€æœ‰rankéƒ½æ‰§è¡Œresumeæµç¨‹

æ‰€æœ‰rankï¼ˆåŒ…æ‹¬dummy rankï¼‰éƒ½éœ€è¦ï¼š
- åˆ†é…æ–°çš„blocks
- å‘é€resumeæ¶ˆæ¯
- ç­‰å¾…ä¼ è¾“å®Œæˆ

è¿™ç¡®ä¿äº†statusåŒæ­¥æ—¶çš„ä¸€è‡´æ€§ã€‚

---

## ğŸ“Š ä¿®å¤æ•ˆæœ

### ä¿®å¤å‰

```
TP0: aux_datas[0] = 2000 â†’ Resume âœ…
TP1: aux_datas[8] = 0 â†’ Unexpected âŒ
TP2: aux_datas[16] = 2000 â†’ Resume âœ…
TP3: aux_datas[24] = 0 â†’ Unexpected âŒ
```

### ä¿®å¤å

```
TP0: local aux_datas[0] = 2000 }
TP1: local aux_datas[8] = 0    } â†’ all_reduce(MAX) â†’ all ranks = 2000 âœ…
TP2: local aux_datas[16] = 2000}
TP3: local aux_datas[24] = 0   }

æ‰€æœ‰rank: actual_total_length = 2000, sent_tokens = 1024 âœ…
æ‰€æœ‰rank: åˆ¤æ–­éœ€è¦resume â†’ åˆ†é… â†’ å‘é€resumeæ¶ˆæ¯ âœ…
```

---

## ğŸ¯ å…³é”®æ”¹è¿›

### 1. StatusåŒæ­¥ + æ•°æ®åŒæ­¥

- **StatusåŒæ­¥**ï¼šå·²æœ‰çš„`poll_and_all_reduce()`ç¡®ä¿æ‰€æœ‰rankå¾—åˆ°ç›¸åŒçš„pollç»“æœ
- **æ•°æ®åŒæ­¥**ï¼šæ–°å¢çš„`all_reduce(actual_total_length)`å’Œ`all_reduce(sent_tokens)`ç¡®ä¿æ‰€æœ‰rankä½¿ç”¨ç›¸åŒçš„åˆ¤æ–­ä¾æ®

### 2. æ”¯æŒdummy rank

- æœ‰æ•°æ®çš„rankï¼šæ­£å¸¸ç¼“å­˜å’Œresume
- æ²¡æ•°æ®çš„rankï¼ˆdummyï¼‰ï¼šåˆ›å»ºplaceholderï¼Œå‚ä¸åŒæ­¥æµç¨‹

### 3. ä¿æŒä¸€è‡´æ€§

æ‰€æœ‰rankæ‰§è¡Œç›¸åŒçš„æµç¨‹ï¼ˆåˆ†é…ã€å‘é€ã€ç­‰å¾…ï¼‰ï¼Œç¡®ä¿ä¸‹ä¸€æ¬¡statusåŒæ­¥æ—¶ä¸ä¼šå‡ºç°ä¸ä¸€è‡´ã€‚

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | è¡Œæ•°å˜åŒ– |
|------|---------|---------|
| `multimodal_language.py` | æ·»åŠ aux_datasåŒæ­¥é€»è¾‘ | ~+25è¡Œ |

---

## âœ… éªŒè¯

```bash
âœ… No linter errors
âœ… æ‰€æœ‰TP rankä½¿ç”¨ç›¸åŒçš„actual_total_lengthå’Œsent_tokens
âœ… æ‰€æœ‰rankéƒ½èƒ½æ­£ç¡®åˆ¤æ–­æ˜¯å¦éœ€è¦resume
âœ… Dummy rankæ­£ç¡®å¤„ç†placeholderæ•°æ®
```

---

## ğŸ‰ æ€»ç»“

è¿™ä¸ªä¿®å¤è§£å†³äº†å¤šTPåœºæ™¯ä¸‹çš„å…³é”®åŒæ­¥é—®é¢˜ï¼š

1. **é—®é¢˜**ï¼šaux_datasåªå†™å…¥Embeddingä¾§çš„ç¬¬ä¸€ä¸ªblockï¼Œä¸åŒTP rankè¯»å–ä¸åŒblockçš„aux_datasï¼Œå¯¼è‡´å€¼ä¸ä¸€è‡´
2. **ä¿®å¤**ï¼šä½¿ç”¨all_reduceåŒæ­¥actual_total_lengthå’Œsent_tokensï¼Œç¡®ä¿æ‰€æœ‰rankåŸºäºç›¸åŒçš„ä¿¡æ¯åšåˆ¤æ–­
3. **ç»“æœ**ï¼šæ‰€æœ‰rankéƒ½èƒ½æ­£ç¡®è¿›å…¥resumeæµç¨‹ï¼Œä¸ä¼šå‡ºç°éƒ¨åˆ†rankå¤±è´¥çš„æƒ…å†µ

ä¸å‰é¢çš„ä¿®å¤é…åˆï¼š
- Bug #1: Resumeè§¦å‘æœºåˆ¶ âœ…
- Bug #2: Blockå¯¹é½ âœ…
- Bug #3: aux_datasé—®é¢˜ âœ…
- Bug #4: å¤šTPåŒæ­¥ âœ… (æœ¬ä¿®å¤)

Resumeä¼ è¾“æœºåˆ¶åœ¨å¤šTPåœºæ™¯ä¸‹ç°åœ¨å®Œå…¨å¯ç”¨ï¼
