# å¤šTP RankåŒæ­¥é—®é¢˜ä¿®å¤

## ğŸ› é—®é¢˜æè¿°

**æŠ¥é”™æ—¥å¿—**ï¼š
```
[TP0] Resume transfer initiated: allocated 2 blocks (16384 tokens) âœ…
[TP2] Resume transfer initiated: allocated 2 blocks (16384 tokens) âœ…
[TP1] Unexpected: Transferring status but sent_tokens=0 >= actual_total_length=0 âŒ
[TP3] Unexpected: Transferring status but sent_tokens=0 >= actual_total_length=0 âŒ
```

**ç”¨æˆ·åé¦ˆ**ï¼šå› ä¸ºaux_dataåªå‘é€äº†ç¬¬ä¸€ä¸ªblock

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### æ ¸å¿ƒé—®é¢˜

**ç”¨æˆ·æŒ‡å‡º**ï¼šEmbeddingä¾§çš„aux_dataåªæœ‰ç¬¬ä¸€ä¸ªblockæ‰æ˜¯æœ‰æ•ˆæ•°æ®ï¼Œåç»­resumeæ”¶åˆ°çš„aux_dataè¦ä»ç¬¬ä¸€æ¬¡ä¼ è¾“å¾—åˆ°çš„æ•°æ®è·å–ã€‚

### é—®é¢˜é“¾æ¡

#### 1. aux_datasåªåœ¨ç¬¬ä¸€æ¬¡ä¼ è¾“æ—¶å‘é€

Connectionå±‚çš„`send_embedding()`ä¸­ï¼š
```python
if buffer_type_idx == 3:  # aux_datas
    if sent_tokens == 0 and block_idx == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡ä¼ è¾“çš„ç¬¬ä¸€ä¸ªblock
        chunk_size = embedding_item_len
    else:
        continue  # Resumeä¼ è¾“è·³è¿‡aux_datas âœ… è¿™æ˜¯å¯¹çš„
```

Embeddingä¾§çš„`set_buf()`ä¸­ï¼š
```python
# Store metadata in first block
if block_idx_pos == 0:
    self.aux_datas[block_id][0] = embed_length  # åªå†™å…¥embedding_indices[0]
```

#### 2. å¤šTPåœºæ™¯ä¸‹çš„blockåˆ†é…

```
ä¸åŒTP rankåˆ†é…ä¸åŒçš„blocksï¼š
TP0: embedding_indices = [0, 1, 2, ..., 7]
TP1: embedding_indices = [8, 9, 10, ..., 15]
TP2: embedding_indices = [16, 17, 18, ..., 23]
TP3: embedding_indices = [24, 25, 26, ..., 31]
```

#### 3. ç¬¬ä¸€æ¬¡ä¼ è¾“æ—¶aux_datasçš„åˆ†å‘

```
Embeddingä¾§ä¼ è¾“åˆ°Languageä¾§æ—¶ï¼š
- aux_datasåªåœ¨ç¬¬ä¸€æ¬¡ä¼ è¾“çš„ç¬¬ä¸€ä¸ªblockä¸­å‘é€
- ä½†ä¸åŒTP rankæ¥æ”¶ä¸åŒçš„blocks
- åªæœ‰æ¥æ”¶åˆ°åŒ…å«aux_datasçš„blockçš„rankæ‰èƒ½è¯»åˆ°æœ‰æ•ˆå€¼

å¯èƒ½çš„æƒ…å†µï¼š
- å¦‚æœaux_datasåœ¨å…¨å±€block 0ï¼Œåªæœ‰TP0èƒ½è¯»åˆ°
- å…¶ä»–TP rankè¯»åˆ°çš„aux_datas[block_indices[0]]æ˜¯0ï¼ˆæœªåˆå§‹åŒ–ï¼‰
```

#### 4. Statusé€šè¿‡all_reduceåŒæ­¥ï¼Œæ‰€æœ‰rankéƒ½æ”¶åˆ°Transferring

```python
# poll_and_all_reduce() ç¡®ä¿æ‰€æœ‰rankå¾—åˆ°ç›¸åŒçš„status
æ‰€æœ‰rank: poll = KVPoll.Transferring
```

#### 5. å„rankè¯»å–è‡ªå·±çš„aux_datas

```python
# Languageä¾§å„rankè¯»å–
TP0: aux_datas = self.aux_datas[0]  â†’ [2000, ...] âœ…
TP1: aux_datas = self.aux_datas[8]  â†’ [0, ...] âŒ
TP2: aux_datas = self.aux_datas[16] â†’ [2000, ...] âœ… (å¯èƒ½æ”¶åˆ°äº†æ•°æ®)
TP3: aux_datas = self.aux_datas[24] â†’ [0, ...] âŒ
```

#### 6. TP1/TP3åˆ¤æ–­é”™è¯¯

```python
actual_total_length = int(aux_datas[0])  # = 0 âŒ
sent_tokens = len(fill_ids)              # = 0 âŒ

if actual_total_length > sent_tokens:    # 0 > 0 = False âŒ
    # ä¸è¿›å…¥resumeæµç¨‹
else:
    # è¾“å‡º: "Unexpected: sent_tokens=0 >= actual_total_length=0"
```

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### æ ¸å¿ƒæ€è·¯

**ä½¿ç”¨all_reduceåŒæ­¥aux_datasä¿¡æ¯ï¼Œç¡®ä¿æ‰€æœ‰rankè·å¾—ä¸€è‡´çš„actual_total_lengthå’Œsent_tokens**

### å…³é”®ç†è§£

1. **ç¬¬ä¸€æ¬¡ä¼ è¾“**ï¼š
   - Embeddingä¾§åœ¨ç¬¬ä¸€ä¸ªblockå‘é€aux_datas
   - ä½†åªæœ‰æŸäº›rankèƒ½è¯»åˆ°ï¼ˆå–å†³äºblockåˆ†é…ï¼‰
   - éœ€è¦åŒæ­¥ç¡®ä¿æ‰€æœ‰rankéƒ½è·å¾—è¿™ä¸ªå€¼

2. **Resumeä¼ è¾“**ï¼š
   - Embeddingä¾§**ä¸å†å‘é€**aux_datasï¼ˆå·²ç»åœ¨ç¬¬ä¸€æ¬¡å‘é€äº†ï¼‰
   - Languageä¾§åº”è¯¥ä½¿ç”¨**ç¼“å­˜çš„partial_aux_datas**
   - ä¸èƒ½ä»æ–°åˆ†é…çš„blocksè¯»å–ï¼ˆé‚£äº›blocksçš„aux_datasæ˜¯0ï¼‰

### å®ç°

#### 1. åŒºåˆ†ç¬¬ä¸€æ¬¡å’ŒResumeçš„Transferring

```python
elif poll == KVPoll.Transferring:
    # Check if we already have cached partial data
    if hasattr(language_req.req, 'partial_aux_datas'):
        # Resume already triggered before, use cached values
        # (Embedding side doesn't send aux_data in resume transfer)
        actual_total_length = int(language_req.req.partial_aux_datas[0])
        sent_tokens = language_req.req.partial_sent_tokens
    else:
        # First time seeing Transferring status - read from buffer
        # Note: aux_data is only valid in the first block from Embedding side
        # In multi-TP scenario, some ranks may not have this block
        embedding_data, fill_ids, mrope_positions, aux_datas = (
            self.metadata_buffers.get_buf(block_indices=block_indices)
        )
        actual_total_length = int(aux_datas[0])  # May be 0 on some ranks
        sent_tokens = len(fill_ids)  # May be 0 on some ranks
        
        # Sync aux_data across all ranks (use MAX to get the valid value)
        import torch.distributed as dist
        if self.gloo_group is not None:
            actual_total_length_tensor = torch.tensor([actual_total_length], dtype=torch.int64)
            sent_tokens_tensor = torch.tensor([sent_tokens], dtype=torch.int64)
            
            dist.all_reduce(actual_total_length_tensor, op=dist.ReduceOp.MAX, group=self.gloo_group)
            dist.all_reduce(sent_tokens_tensor, op=dist.ReduceOp.MAX, group=self.gloo_group)
            
            actual_total_length = int(actual_total_length_tensor.item())
            sent_tokens = int(sent_tokens_tensor.item())
    
    # Now all ranks have the same values âœ…
    if actual_total_length > sent_tokens:
        # Cache partial data (first time only)
        if not hasattr(language_req.req, 'partial_input_embeds'):
            # Get data from buffer
            embedding_data, fill_ids, mrope_positions, aux_datas = (
                self.metadata_buffers.get_buf(block_indices=block_indices)
            )
            # Cache for resume (Embedding won't send aux_data again)
            language_req.req.partial_aux_datas = torch.tensor([actual_total_length, ...])
            # ... cache other data
        
        # Resume...
```

#### 2. ç¼“å­˜ç¬¬ä¸€æ¬¡ä¼ è¾“çš„aux_datas

```python
# Cache partial data (first time only)
if not hasattr(language_req.req, 'partial_input_embeds'):
    # Get data from buffer
    embedding_data, fill_ids, mrope_positions, aux_datas = (
        self.metadata_buffers.get_buf(block_indices=block_indices)
    )
    
    # Cache for resume (use synced actual_total_length, not local aux_datas)
    language_req.req.partial_input_embeds = embedding_data
    language_req.req.partial_fill_ids = fill_ids.tolist()
    language_req.req.partial_mrope_positions = mrope_positions
    language_req.req.partial_aux_datas = torch.tensor([actual_total_length, aux_datas[1]])
    language_req.req.partial_sent_tokens = sent_tokens
```

**å…³é”®**ï¼šç¼“å­˜çš„`partial_aux_datas[0]`ä½¿ç”¨åŒæ­¥åçš„`actual_total_length`ï¼Œè€Œä¸æ˜¯æœ¬åœ°è¯»å–çš„å€¼ã€‚

#### 3. Resumeæ—¶ä½¿ç”¨ç¼“å­˜çš„aux_datas

åœ¨åç»­çš„TransferringçŠ¶æ€ï¼ˆå¦‚æœå‘ç”Ÿï¼‰ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼š
```python
if hasattr(language_req.req, 'partial_aux_datas'):
    actual_total_length = int(language_req.req.partial_aux_datas[0])  # ä½¿ç”¨ç¼“å­˜
    sent_tokens = language_req.req.partial_sent_tokens
```

ä¸å†ä»æ–°blocksè¯»å–ï¼ˆé‚£äº›blocksæ²¡æœ‰aux_dataï¼‰ã€‚

---

## ğŸ“Š ä¿®å¤æ•ˆæœ

### ä¿®å¤å‰

```
TP0: aux_datas[0] = 2000 â†’ Resume âœ…
TP1: aux_datas[8] = 0 â†’ Unexpected âŒ
TP2: aux_datas[16] = 2000 â†’ Resume âœ…
TP3: aux_datas[24] = 0 â†’ Unexpected âŒ
```

### ä¿®å¤å

```
TP0: local aux_datas[0] = 2000 }
TP1: local aux_datas[8] = 0    } â†’ all_reduce(MAX) â†’ all ranks = 2000 âœ…
TP2: local aux_datas[16] = 2000}
TP3: local aux_datas[24] = 0   }

æ‰€æœ‰rank: actual_total_length = 2000, sent_tokens = 1024 âœ…
æ‰€æœ‰rank: åˆ¤æ–­éœ€è¦resume â†’ åˆ†é… â†’ å‘é€resumeæ¶ˆæ¯ âœ…
```

---

## ğŸ¯ å…³é”®æ”¹è¿›

### 1. StatusåŒæ­¥ + æ•°æ®åŒæ­¥

- **StatusåŒæ­¥**ï¼šå·²æœ‰çš„`poll_and_all_reduce()`ç¡®ä¿æ‰€æœ‰rankå¾—åˆ°ç›¸åŒçš„pollç»“æœ
- **æ•°æ®åŒæ­¥**ï¼šæ–°å¢çš„`all_reduce(actual_total_length)`å’Œ`all_reduce(sent_tokens)`ç¡®ä¿æ‰€æœ‰rankä½¿ç”¨ç›¸åŒçš„åˆ¤æ–­ä¾æ®

### 2. æ”¯æŒdummy rank

- æœ‰æ•°æ®çš„rankï¼šæ­£å¸¸ç¼“å­˜å’Œresume
- æ²¡æ•°æ®çš„rankï¼ˆdummyï¼‰ï¼šåˆ›å»ºplaceholderï¼Œå‚ä¸åŒæ­¥æµç¨‹

### 3. ä¿æŒä¸€è‡´æ€§

æ‰€æœ‰rankæ‰§è¡Œç›¸åŒçš„æµç¨‹ï¼ˆåˆ†é…ã€å‘é€ã€ç­‰å¾…ï¼‰ï¼Œç¡®ä¿ä¸‹ä¸€æ¬¡statusåŒæ­¥æ—¶ä¸ä¼šå‡ºç°ä¸ä¸€è‡´ã€‚

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | è¡Œæ•°å˜åŒ– |
|------|---------|---------|
| `multimodal_language.py` | æ·»åŠ aux_datasåŒæ­¥é€»è¾‘ | ~+25è¡Œ |

---

## âœ… éªŒè¯

```bash
âœ… No linter errors
âœ… æ‰€æœ‰TP rankä½¿ç”¨ç›¸åŒçš„actual_total_lengthå’Œsent_tokens
âœ… æ‰€æœ‰rankéƒ½èƒ½æ­£ç¡®åˆ¤æ–­æ˜¯å¦éœ€è¦resume
âœ… Dummy rankæ­£ç¡®å¤„ç†placeholderæ•°æ®
```

---

## ğŸ‰ æ€»ç»“

è¿™ä¸ªä¿®å¤è§£å†³äº†å¤šTPåœºæ™¯ä¸‹çš„å…³é”®åŒæ­¥é—®é¢˜ï¼š

1. **é—®é¢˜**ï¼šaux_datasåªå†™å…¥Embeddingä¾§çš„ç¬¬ä¸€ä¸ªblockï¼Œä¸åŒTP rankè¯»å–ä¸åŒblockçš„aux_datasï¼Œå¯¼è‡´å€¼ä¸ä¸€è‡´
2. **ä¿®å¤**ï¼šä½¿ç”¨all_reduceåŒæ­¥actual_total_lengthå’Œsent_tokensï¼Œç¡®ä¿æ‰€æœ‰rankåŸºäºç›¸åŒçš„ä¿¡æ¯åšåˆ¤æ–­
3. **ç»“æœ**ï¼šæ‰€æœ‰rankéƒ½èƒ½æ­£ç¡®è¿›å…¥resumeæµç¨‹ï¼Œä¸ä¼šå‡ºç°éƒ¨åˆ†rankå¤±è´¥çš„æƒ…å†µ

ä¸å‰é¢çš„ä¿®å¤é…åˆï¼š
- Bug #1: Resumeè§¦å‘æœºåˆ¶ âœ…
- Bug #2: Blockå¯¹é½ âœ…
- Bug #3: aux_datasé—®é¢˜ âœ…
- Bug #4: å¤šTPåŒæ­¥ âœ… (æœ¬ä¿®å¤)

Resumeä¼ è¾“æœºåˆ¶åœ¨å¤šTPåœºæ™¯ä¸‹ç°åœ¨å®Œå…¨å¯ç”¨ï¼
