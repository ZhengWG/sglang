# Mini Load Balancer Quick Start Guide

## Overview

Mini Load Balancer (Mini LB) is a lightweight Rust-based load balancer specifically designed for SGLang's Prefill-Decode disaggregation architecture.

## Quick Start

### 1. Start Prefill Servers

```bash
# Start the first Prefill server
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B-Instruct \
  --port 30000 \
  --host 0.0.0.0 \
  --disagg-prefill-only \
  --bootstrap-port 30001

# (Optional) Start the second Prefill server
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B-Instruct \
  --port 30002 \
  --host 0.0.0.0 \
  --disagg-prefill-only \
  --bootstrap-port 30003
```

### 2. Start Decode Servers

```bash
# Start the first Decode server
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B-Instruct \
  --port 31000 \
  --host 0.0.0.0 \
  --disagg-decode-only

# (Optional) Start the second Decode server
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B-Instruct \
  --port 31001 \
  --host 0.0.0.0 \
  --disagg-decode-only
```

### 3. Start Mini Load Balancer

```bash
# Method 1: Run basic example
cd sgl-router
cargo run --example mini_lb_basic

# Method 2: Run custom config example
cargo run --example mini_lb_custom_config
```

### 4. Send Test Requests

#### Using cURL

```bash
# Health check
curl http://localhost:8080/health

# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "user", "content": "What is Rust?"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Streaming chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "user", "content": "Count from 1 to 5"}
    ],
    "stream": true,
    "max_tokens": 50
  }'

# Text generation
curl -X POST http://localhost:8080/generate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Once upon a time",
    "sampling_params": {
      "temperature": 0.8,
      "max_new_tokens": 50
    }
  }'
```

#### Using Python

```bash
# Run Python test client
cd sgl-router/examples
python mini_lb_python_example.py
```

#### Using Rust Client

```bash
cd sgl-router
cargo run --example mini_lb_client
```

## Configuration Guide

### Basic Configuration

```rust
let config = MiniLbConfig::new(
    "0.0.0.0".to_string(),  // ç›‘å¬åœ°å€
    8080,                   // ç›‘å¬ç«¯å£
    vec![                   // Prefill æœåŠ¡å™¨åˆ—è¡¨
        ("http://localhost:30000".to_string(), Some(30001)),
    ],
    vec![                   // Decode æœåŠ¡å™¨åˆ—è¡¨
        "http://localhost:31000".to_string(),
    ],
    1800,                   // è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
);
```

### å¤šæœåŠ¡å™¨é…ç½®

```rust
let config = MiniLbConfig::new(
    "0.0.0.0".to_string(),
    8080,
    vec![
        ("http://prefill-1:30000".to_string(), Some(30001)),
        ("http://prefill-2:30000".to_string(), Some(30001)),
        ("http://prefill-3:30000".to_string(), Some(30001)),
    ],
    vec![
        "http://decode-1:31000".to_string(),
        "http://decode-2:31000".to_string(),
        "http://decode-3:31000".to_string(),
    ],
    3600,
);
```

## æ”¯æŒçš„ç«¯ç‚¹

### å¥åº·æ£€æŸ¥
- `GET /health` - åŸºç¡€å¥åº·æ£€æŸ¥
- `GET /health_generate` - åç«¯æœåŠ¡å™¨å¥åº·æ£€æŸ¥

### ç®¡ç†ç«¯ç‚¹
- `POST /flush_cache` - åˆ·æ–°ç¼“å­˜
- `GET /get_server_info` - è·å–æœåŠ¡å™¨ä¿¡æ¯
- `GET /get_model_info` - è·å–æ¨¡å‹ä¿¡æ¯

### OpenAI API
- `POST /v1/chat/completions` - èŠå¤©è¡¥å…¨
- `POST /v1/completions` - æ–‡æœ¬è¡¥å…¨
- `GET /v1/models` - åˆ—å‡ºæ¨¡å‹

### SGLang API
- `POST /generate` - æ–‡æœ¬ç”Ÿæˆ

## å¸¸è§é—®é¢˜

### 1. è´Ÿè½½å‡è¡¡å™¨æ— æ³•å¯åŠ¨
- æ£€æŸ¥ç«¯å£ 8080 æ˜¯å¦è¢«å ç”¨
- éªŒè¯é…ç½®ä¸­çš„ä¸»æœºå’Œç«¯å£è®¾ç½®
- æŸ¥çœ‹æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯

### 2. è¯·æ±‚è¶…æ—¶
- å¢åŠ é…ç½®ä¸­çš„ `timeout_secs`
- ç¡®è®¤åç«¯æœåŠ¡å™¨æ­£åœ¨è¿è¡Œä¸”å¯è®¿é—®
- æ£€æŸ¥ç½‘ç»œè¿æ¥

### 3. "æ²¡æœ‰å¯ç”¨æœåŠ¡å™¨"é”™è¯¯
- ç¡®ä¿ `prefill_urls` å’Œ `decode_urls` ä¸ä¸ºç©º
- éªŒè¯æœåŠ¡å™¨ URL æ ¼å¼æ­£ç¡®
- ç¡®è®¤åç«¯æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ

## æ€§èƒ½æç¤º

1. **æœåŠ¡å™¨æ•°é‡**: æ ¹æ®è´Ÿè½½è°ƒæ•´ Prefill å’Œ Decode æœåŠ¡å™¨æ•°é‡
2. **è¶…æ—¶è®¾ç½®**: æ ¹æ®æ¨¡å‹å¤§å°å’Œè¯·æ±‚å¤æ‚åº¦è°ƒæ•´è¶…æ—¶æ—¶é—´
3. **å¹¶å‘è¿æ¥**: Mini LB è‡ªåŠ¨å¤„ç†å¹¶å‘è¯·æ±‚
4. **ç›‘æ§**: ä½¿ç”¨ `/get_server_info` ç«¯ç‚¹ç›‘æ§æœåŠ¡å™¨çŠ¶æ€

## ä¸‹ä¸€æ­¥

- é˜…è¯»[å®Œæ•´æ–‡æ¡£](./mini_lb_README.md)
- æŸ¥çœ‹[å®ç°æ€»ç»“](./MINI_LB_IMPLEMENTATION_SUMMARY.md)
- å‚è€ƒç¤ºä¾‹ä»£ç å­¦ä¹ æ›´å¤šç”¨æ³•
- æ ¹æ®éœ€æ±‚è°ƒæ•´é…ç½®

## è·å–å¸®åŠ©

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æ—¥å¿—è¾“å‡º
2. æ£€æŸ¥åç«¯æœåŠ¡å™¨çŠ¶æ€
3. éªŒè¯ç½‘ç»œè¿æ¥
4. æäº¤ GitHub issue

---

Happy coding! ğŸš€
