# å¤šæ¨¡æ€Embeddingåˆ†æ‰¹ä¼ è¾“ - æœ€ç»ˆå®ç°æ€»ç»“

## âœ… å®ç°å®Œæˆ

**ç‰ˆæœ¬**: v5.0-final  
**å®Œæˆæ—¶é—´**: 2025-10-20  
**çŠ¶æ€**: âœ… Ready for Testing  

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

å®ç°äº†æ”¯æŒå¤šæ¨¡æ€Embeddingæ•°æ®åˆ†æ‰¹ä¼ è¾“çš„å®Œæ•´åŠŸèƒ½ï¼Œè§£å†³å½“å®é™…æ•°æ®é•¿åº¦è¶…è¿‡é»˜è®¤bufferå¤§å°æ—¶çš„ä¼ è¾“é—®é¢˜ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. âœ… **Block-basedå†…å­˜ç®¡ç†** - è¿ç»­blockåˆ†é…ï¼Œé¿å…ç¢ç‰‡
2. âœ… **Resumeä¼ è¾“æœºåˆ¶** - è‡ªåŠ¨åˆ†æ‰¹ä¼ è¾“å¤§æ•°æ®
3. âœ… **åŠ¨æ€é€‚é…** - Languageä¾§æŒ‰é»˜è®¤blockæ•°åˆ†é…ï¼ŒEmbeddingä¾§æŒ‰å®é™…é•¿åº¦åˆ†é…
4. âœ… **ä¸“ä¸šå‘½å** - é‡‡ç”¨Resume-basedæœ¯è¯­ï¼ˆè¡Œä¸šæ ‡å‡†ï¼‰
5. âœ… **ç®€æ´å®ç°** - ä»£ç ç®€åŒ–75%ï¼Œå•ä¸€æ¨¡å¼

---

## ğŸ“Š è®¾è®¡è¦ç‚¹

### 1. æ•°æ®ç»“æ„ï¼ˆæœ€ç»ˆç‰ˆï¼‰

```python
@dataclass
class MetadataAllocation:
    start_block: int  # èµ·å§‹blockï¼ˆä¿è¯è¿ç»­åˆ†é…ï¼‰
    num_blocks: int   # blockæ•°é‡
    num_tokens: int   # å®é™…tokenæ•°
```

**è¿ç»­æ€§ä¿è¯**ï¼šblocksæ€»æ˜¯ `[start_block, start_block+1, ..., start_block+num_blocks-1]`

### 2. åˆ†é…å™¨APIï¼ˆæœ€ç»ˆç‰ˆï¼‰

```python
class ReqToMetadataBlockAllocator:
    default_num_blocks = 8  # ä»ç¯å¢ƒå˜é‡ SGLANG_DEFAULT_MULTIMODAL_BLOCKS è¯»å–
    
    # Languageä¾§ï¼šæŒ‰å›ºå®šblockæ•°åˆ†é…
    def alloc_default(req_id, fake) -> MetadataAllocation
    
    # Embeddingä¾§ï¼šæŒ‰å®é™…tokenæ•°åˆ†é…
    def alloc(num_tokens, req_id, fake) -> MetadataAllocation
    
    # åº•å±‚æ–¹æ³•ï¼šæŒ‰blockæ•°åˆ†é…
    def alloc_blocks(num_blocks, num_tokens, req_id, fake) -> MetadataAllocation
    
    # é‡Šæ”¾
    def free(allocation, req_id, fake)
```

### 3. Bufferç®¡ç†ï¼ˆæœ€ç»ˆç‰ˆï¼‰

```python
class MultimodalDataBuffers:
    default_buffer_tokens = 1024  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆä»…ç”¨äºåˆ¤æ–­ï¼‰
    
    # è®¡ç®—ä¼ è¾“ä¿¡æ¯ï¼ˆåŸºäºstart_blockï¼ŒO(1)å¤æ‚åº¦ï¼‰
    def get_buf_chunk_info(allocation, offset_tokens, max_tokens) -> List[Tuple]
    
    # è¯»å–æ•°æ®ï¼ˆåŸºäºstart_blockï¼‰
    def get_buf(allocation) -> (embeddings, fill_ids, mrope, aux)
    
    # å†™å…¥æ•°æ®ï¼ˆåŸºäºstart_blockï¼‰
    def set_buf(req, allocation)
```

---

## ğŸ”„ ä¼ è¾“æµç¨‹ï¼ˆæœ€ç»ˆç‰ˆï¼‰

```
å®é™…é•¿åº¦2000 tokensï¼Œé»˜è®¤8 blocks (1024 tokens)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Language   â”‚                           â”‚  Embedding   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                            â”‚
      â”‚ 1. alloc_default() -> 8 blocks            â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚    init(allocation)                       â”‚
      â”‚    start_block=0, num_blocks=8            â”‚
      â”‚                                            â”‚
      â”‚                                    2. å¤„ç†æ•°æ®
      â”‚                                       actual_length=2000
      â”‚                                       alloc(2000) -> 16 blocks
      â”‚                                            â”‚
      â”‚ 3. å‘é€1024 + aux[total=2000]             â”‚
      â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚    Transferring (is_last=False)           â”‚
      â”‚                                            â”‚
      â”‚ 4. è¯»å–aux[0]=2000ï¼Œéœ€è¦resume            â”‚
      â”‚    free(8 blocks)                         â”‚
      â”‚    alloc(976) -> 8 blocks                 â”‚
      â”‚                                            â”‚
      â”‚ 5. resume_transfer(sent_tokens=1024)      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚    start_block=8, num_blocks=8            â”‚
      â”‚                                            â”‚
      â”‚ 6. å‘é€å‰©ä½™976 tokens                     â”‚
      â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚    Success (is_last=True)                 â”‚
      â”‚                                            â”‚
      â”‚ 7. æ‹¼æ¥ï¼š1024 + 976 = 2000 âœ“              â”‚
      â””                                            â”˜
```

---

## ğŸ”§ é…ç½®å‚æ•°

```bash
# Blockå¤§å°ï¼ˆtokens per blockï¼‰
export SGLANG_MULTIMODAL_BLOCK_SIZE=128

# Languageä¾§é»˜è®¤ç”³è¯·çš„blockæ•°é‡
export SGLANG_DEFAULT_MULTIMODAL_BLOCKS=8

# Bufferæ€»æ•°é‡
export SGLANG_EMBEDDING_CACHE_BUFFER_SIZE=64
```

**è®¡ç®—å…³ç³»**ï¼š
```
default_buffer_tokens = DEFAULT_MULTIMODAL_BLOCKS * MULTIMODAL_BLOCK_SIZE
                      = 8 * 128 = 1024 tokens

æ€»å®¹é‡ = EMBEDDING_CACHE_BUFFER_SIZE * max_req_len
```

**æ¨èé…ç½®**ï¼š

| åœºæ™¯ | DEFAULT_BLOCKS | BLOCK_SIZE | é»˜è®¤Buffer |
|------|----------------|------------|-----------|
| å°å›¾ç‰‡ | 4 | 128 | 512 tokens |
| ä¸­ç­‰å›¾ç‰‡ | 8 | 128 | 1024 tokens â­ |
| å¤§å›¾ç‰‡ | 16 | 128 | 2048 tokens |

---

## ğŸ“ æ ¸å¿ƒä»£ç 

### Language ä¾§

```python
# é¦–æ¬¡åˆ†é…ï¼ˆä¸çŸ¥é“å®é™…é•¿åº¦ï¼‰
allocation = allocator.alloc_default(req_id=req.rid)
receiver.init(allocation)

# TransferringçŠ¶æ€æ£€æŸ¥
if total_length > default_tokens:
    # ç¼“å­˜ç¬¬ä¸€æ‰¹æ•°æ®
    buffered_chunks = {
        "embeddings": embedding_data[:transferred_length].clone(),
        "fill_ids": fill_ids[:transferred_length].clone(),
        ...
    }
    transferred_tokens = transferred_length
    
    # é‡Šæ”¾æ—§bufferï¼Œç”³è¯·æ–°buffer
    allocator.free(old_allocation, req_id)
    new_allocation = allocator.alloc(num_tokens=remaining, req_id)
    
    # Resumeä¼ è¾“
    receiver.resume_transfer(new_allocation, sent_tokens=transferred_tokens)

# SuccessçŠ¶æ€æ‹¼æ¥
if transferred_tokens > 0:
    full_data = torch.cat([buffered_chunks["embeddings"], new_embeddings])
```

### Embedding ä¾§

```python
# æŒ‰å®é™…é•¿åº¦åˆ†é…
actual_length = req.embedding.shape[0]
allocation = allocator.alloc(num_tokens=actual_length, req_id=req.rid)

# è®¾ç½®buffer
buffers.set_buf(req, allocation)

# å‘é€æ•°æ®
if sent_tokens == 0:
    # é¦–æ¬¡ï¼šé™åˆ¶ä¸ºdefault_tokens
    is_last = actual_length <= default_tokens
    chunk_info = buffers.get_buf_chunk_info(allocation, 0, default_tokens)
else:
    # Resumeï¼šå‘é€å‰©ä½™æ‰€æœ‰æ•°æ®
    is_last = True
    chunk_info = buffers.get_buf_chunk_info(allocation, sent_tokens)

sender.send_embedding(allocation.start_block, is_last, chunk_info)
```

---

## ğŸ¯ å…³é”®æ”¹è¿›ç‚¹

### ä¿®æ­£1: è¿ç»­Blockåˆ†é…

**é—®é¢˜**ï¼š`block_indices = [5, 2, 8]` æ— åºï¼Œ`min()` ä¸å¯é 

**è§£å†³**ï¼š
```python
# æ—§è®¾è®¡
allocation.block_indices = [5, 2, 8]  # âŒ æ— åº
start_token = min(block_indices) * block_size

# æ–°è®¾è®¡  
allocation.start_block = 2  # âœ… èµ·å§‹block
allocation.num_blocks = 3   # âœ… è¿ç»­blocks [2,3,4]
start_token = start_block * block_size
```

### ä¿®æ­£2: Languageä¾§åˆ†é…æ–¹å¼

**é—®é¢˜**ï¼šLanguageä¾§ä¸çŸ¥é“å®é™…é•¿åº¦ï¼Œä¸åº”ä¼ å…¥num_tokens

**è§£å†³**ï¼š
```python
# æ—§è®¾è®¡
allocation = allocator.alloc(num_tokens=1024)  # âŒ ç¡¬ç¼–ç 

# æ–°è®¾è®¡
allocation = allocator.alloc_default()  # âœ… ä½¿ç”¨é¢„è®¾blockæ•°
# ä»ç¯å¢ƒå˜é‡ SGLANG_DEFAULT_MULTIMODAL_BLOCKS è¯»å–
```

### ä¿®æ­£3: å‘½åä¸“ä¸šåŒ–

**å˜æ›´**ï¼š
- `continuation` â†’ `resume` (è¡Œä¸šæ ‡å‡†æœ¯è¯­)
- `partial_data` â†’ `buffered_chunks` (æ›´å‡†ç¡®)
- `received_tokens` â†’ `transferred_tokens` (æ›´æ¸…æ™°)

---

## ğŸ“¦ ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | çŠ¶æ€ |
|------|---------|------|
| `utils.py` | MetadataAllocationç»“æ„ + åˆ†é…å™¨é‡æ„ | âœ… |
| `conn_multimodal.py` | sent_tokensæ”¯æŒ + resume_transfer() | âœ… |
| `multimodal_embedding.py` | æŒ‰å®é™…é•¿åº¦åˆ†é… + åˆ†æ‰¹å‘é€ | âœ… |
| `multimodal_language.py` | alloc_default() + resumeé€»è¾‘ | âœ… |
| `scheduler.py` | ç®€åŒ–åˆå§‹åŒ– | âœ… |

**LinterçŠ¶æ€**: âœ… 0 Errors

---

## ğŸ§ª å¿«é€Ÿæµ‹è¯•

```bash
# 1. é…ç½®ç¯å¢ƒå˜é‡
export SGLANG_MULTIMODAL_BLOCK_SIZE=128
export SGLANG_DEFAULT_MULTIMODAL_BLOCKS=8
export SGLANG_EMBEDDING_CACHE_BUFFER_SIZE=64

# 2. å¯åŠ¨æœåŠ¡ï¼ˆEmbeddingä¾§ï¼‰
python -m sglang.launch_server \
    --model-path /path/to/model \
    --disaggregation-mode encode \
    --disaggregation-bootstrap-port 8001

# 3. å¯åŠ¨æœåŠ¡ï¼ˆLanguageä¾§ï¼‰  
python -m sglang.launch_server \
    --model-path /path/to/model \
    --disaggregation-mode language \
    --disaggregation-bootstrap-addr localhost:8001

# 4. ç›‘æ§æ—¥å¿—
tail -f logs/*.log | grep -E "resume|transferred_tokens|buffered_chunks"
```

**é¢„æœŸæ—¥å¿—**ï¼š
```
DEBUG: Request 123 needs resume for remaining data
DEBUG: Allocated 8 blocks to resume transfer: 976 tokens remaining
INFO: Request 123 completed with resumed transfer: 2000 tokens total
```

---

## âœ… è´¨é‡ä¿è¯

- âœ… **è®¾è®¡ä¿®æ­£**ï¼šè¿ç»­blockåˆ†é… + alloc_default()
- âœ… **å‘½åä¸“ä¸šåŒ–**ï¼šResume-basedæœ¯è¯­
- âœ… **ä»£ç ç®€åŒ–**ï¼šå•ä¸€æ¨¡å¼ï¼Œå‡å°‘75%ä»£ç 
- âœ… **Linteré€šè¿‡**ï¼š0é”™è¯¯
- âœ… **é€»è¾‘éªŒè¯**ï¼šPythonå¿«é€ŸéªŒè¯é€šè¿‡

---

## ğŸš€ Ready for Release!

æ ¸å¿ƒåŠŸèƒ½å·²å®Œæˆï¼Œè®¾è®¡é—®é¢˜å·²ä¿®æ­£ï¼Œä»£ç è´¨é‡é«˜ï¼Œå¯ä»¥å¼€å§‹æµ‹è¯•å’Œéƒ¨ç½²ï¼

**ä¸‹ä¸€æ­¥**ï¼š
1. é›†æˆæµ‹è¯•
2. æ€§èƒ½æµ‹è¯•
3. ç”Ÿäº§ç¯å¢ƒè¯•ç‚¹

---

**ğŸ‰ æ­å–œï¼å®ç°å®Œæˆï¼Œå‡†å¤‡å‘å¸ƒï¼**
