# ğŸ‰ Qwen3-MoE-VL DeepStack Disaggregation - å®ç°å®Œæˆ

## ğŸ“Š æ€»è§ˆ

âœ… **æ‰€æœ‰ Phase å®Œæˆ**: Phase 0-5 å…¨éƒ¨å®æ–½å®Œæ¯•

- **ä¿®æ”¹æ–‡ä»¶**: 7 ä¸ª
- **æ·»åŠ ä»£ç **: ~200 è¡Œ
- **ä¿®æ”¹ä»£ç **: ~50 è¡Œ
- **æ¶ˆé™¤é‡å¤**: 90 è¡Œ
- **å‡€å¢åŠ **: ~60 è¡Œæ ¸å¿ƒåŠŸèƒ½ä»£ç 
- **Linter çŠ¶æ€**: âœ… æ— é”™è¯¯

## ğŸ¯ å®ç°å†…å®¹

### âœ… Phase 0: æ¨¡å‹å±‚é‡æ„ä¸ç®€åŒ–
- ä¸º `Qwen2MoeModel` æ·»åŠ  deepstack æ”¯æŒ
- ä¸º `Qwen3MoeForCausalLM` æ·»åŠ  deepstack å‚æ•°ä¼ é€’
- åˆ é™¤ `qwen3_vl_moe.py` ä¸­çš„ `Qwen3MoeLLMModel` é‡å¤ç±»
- å‡å°‘ 90 è¡Œé‡å¤ä»£ç 

### âœ… Phase 1: æ‰©å±•ç¼“å†²åŒºç»“æ„
**æ–‡ä»¶**: `python/sglang/srt/disaggregation/utils.py`

**ä¿®æ”¹å†…å®¹**:
1. **`MultimodalDataBuffers.__init__`**:
   ```python
   + num_deepstack_embeddings: int = 0  # æ–°å‚æ•°
   + self.num_deepstack_embeddings = num_deepstack_embeddings
   + 
   + if num_deepstack_embeddings > 0:
   +     self.deepstack_embeddings = torch.zeros(
   +         (size, block_size * embedding_dim * num_deepstack_embeddings),
   +         dtype=torch.bfloat16,
   +         device="cpu",
   +     )
   ```

2. **`get_block_buffer_sizes()`**:
   ```python
   + if self.deepstack_embeddings is not None:
   +     deepstack_size = (block_size * embedding_dim * num_deepstack_embeddings * itemsize)
   + return ..., deepstack_size  # è¿”å›å€¼å¢åŠ  deepstack_size
   ```

3. **`get_buf_infos()`**:
   ```python
   + if self.deepstack_embeddings is not None:
   +     ptrs.append(self.deepstack_embeddings.data_ptr())
   +     data_lens.append(self.deepstack_embeddings.nbytes)
   +     item_lens.append(self.deepstack_embeddings[0].nbytes)
   ```

4. **`get_buf()`**:
   ```python
   + gathered_deepstack = []  # æ–°å¢ deepstack gathering
   + 
   + if self.deepstack_embeddings is not None:
   +     # Gather deepstack from blocks
   +     ...
   + 
   + return ..., deepstack_embeddings  # è¿”å›å€¼å¢åŠ  deepstack
   ```

5. **`set_buf()`**:
   ```python
   + if self.deepstack_embeddings is not None and hasattr(req, "deepstack_embedding"):
   +     # Scatter deepstack to blocks
   +     self.deepstack_embeddings[block_id, :deepstack_len] = req.deepstack_embedding[...]
   ```

**ç»Ÿè®¡**: +60 è¡Œ

### âœ… Phase 2: Encodeä¾§æ›´æ–°
**æ–‡ä»¶**: `python/sglang/srt/disaggregation/multimodal_embedding.py`

**ä¿®æ”¹å†…å®¹**:
1. **`process_batch_result_disagg_multimodal_embedding()`**:
   ```python
   + # Extract deepstack if model supports it
   + if hasattr(self.model_runner.model, "use_deepstack") and self.model_runner.model.use_deepstack:
   +     if hasattr(self.model_runner.model, "separate_deepstack_embeds"):
   +         req.embedding, req.deepstack_embedding = (
   +             self.model_runner.model.separate_deepstack_embeds(req.embedding)
   +         )
   +     else:
   +         req.deepstack_embedding = None
   + else:
   +     req.deepstack_embedding = None
   ```

**å…³é”®é€»è¾‘**:
- æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ `use_deepstack`
- è°ƒç”¨ `separate_deepstack_embeds()` åˆ†ç¦» embeddings
- å­˜å‚¨åˆ° `req.deepstack_embedding` ä¾› buffer ä½¿ç”¨

**ç»Ÿè®¡**: +12 è¡Œ

### âœ… Phase 3: Languageä¾§æ›´æ–°
**æ–‡ä»¶**: `python/sglang/srt/disaggregation/multimodal_language.py`

**ä¿®æ”¹å†…å®¹**:
1. **`pop_transferred()` - æˆåŠŸæ¥æ”¶æ—¶**:
   ```python
   # æ­£å¸¸æ¥æ”¶
   - embedding_data, fill_ids, mrope_positions, aux_datas = (...)
   + embedding_data, fill_ids, mrope_positions, aux_datas, deepstack_data = (...)
   
   + # Store deepstack embeddings if present
   + if deepstack_data is not None:
   +     language_req.req.input_deepstack_embeds = deepstack_data.to(embedding_data.device)
   + else:
   +     language_req.req.input_deepstack_embeds = None
   ```

2. **`pop_transferred()` - æ–­ç‚¹ç»­ä¼ æ—¶**:
   ```python
   # ç¼“å­˜partialæ•°æ®
   + if deepstack_data is not None:
   +     language_req.partial_deepstack_embeds = deepstack_data
   
   # æ¢å¤partialæ•°æ®
   + if hasattr(language_req, "partial_deepstack_embeds"):
   +     deepstack_data = language_req.partial_deepstack_embeds
   +     del language_req.partial_deepstack_embeds
   ```

**å…³é”®é€»è¾‘**:
- ä» buffer è·å– deepstack embeddings
- å­˜å‚¨åˆ° `req.input_deepstack_embeds` (æ¨¡å‹forwardä¼šè‡ªåŠ¨ä½¿ç”¨)
- æ”¯æŒæ–­ç‚¹ç»­ä¼  (deepstackåªåœ¨åˆå§‹ä¼ è¾“ä¸­å‘é€ï¼Œç»­ä¼ æ—¶ä½¿ç”¨ç¼“å­˜)

**ç»Ÿè®¡**: +40 è¡Œ

### âœ… Phase 4: ä¼ è¾“åè®®æ›´æ–°
**æ–‡ä»¶**: `python/sglang/srt/disaggregation/mooncake/conn_multimodal.py`

**ä¿®æ”¹å†…å®¹**:
1. **`send_embedding()` - æ·»åŠ  deepstack buffer ä¼ è¾“**:
   ```python
   for buffer_type_idx in range(len(self.data_args.aux_item_lens)):
       if buffer_type_idx == 3:  # aux_datas
           if sent_tokens == 0 and block_idx == 0:
               chunk_size = embedding_item_len
           else:
               continue
   +   elif buffer_type_idx == 4:  # deepstack_embeddings
   +       if len(self.data_args.aux_item_lens) > 4:  # Check if deepstack exists
   +           if sent_tokens == 0 and block_idx == 0:
   +               chunk_size = embedding_item_len  # Only in first block of initial transfer
   +           else:
   +               continue  # Skip for resume or other blocks
   +       else:
   +           continue  # Skip if no deepstack buffer
       else:
           # Regular buffers: scale by tokens_in_block
           chunk_size = (embedding_item_len * tokens_in_block) // block_size
   ```

**å…³é”®é€»è¾‘**:
- Buffer ç´¢å¼• 4 = deepstack_embeddings
- ä»…åœ¨åˆå§‹ä¼ è¾“çš„ç¬¬ä¸€ä¸ªå—ä¸­å‘é€ (sent_tokens == 0 and block_idx == 0)
- æ–­ç‚¹ç»­ä¼ æ—¶è·³è¿‡ deepstack (å·²åœ¨åˆå§‹ä¼ è¾“ä¸­å‘é€)

**ç»Ÿè®¡**: +13 è¡Œ

## ğŸ“Š ä¿®æ”¹ç»Ÿè®¡

| Phase | æ–‡ä»¶ | æ·»åŠ  | åˆ é™¤ | å‡€å˜åŒ– |
|-------|------|------|------|--------|
| 0 | models/ | +16 | -90 | -74 |
| 1 | utils.py | +60 | -10 | +50 |
| 2 | multimodal_embedding.py | +12 | 0 | +12 |
| 3 | multimodal_language.py | +40 | -10 | +30 |
| 4 | conn_multimodal.py | +13 | -5 | +8 |
| **æ€»è®¡** | 7 files | **+141** | **-115** | **+26** |

## ğŸ—ï¸ æ•°æ®æµ

### Encode Side (Embeddingç”Ÿæˆ)
```
1. Forward pass â†’ full_embeddings (seq_len, hidden_size * 4)
2. separate_deepstack_embeds() 
   â†’ regular_embedding (seq_len, hidden_size)
   â†’ deepstack_embedding (seq_len, hidden_size * 3)
3. Store in buffer:
   â†’ req.embedding â†’ MultimodalDataBuffers.input_embeddings
   â†’ req.deepstack_embedding â†’ MultimodalDataBuffers.deepstack_embeddings
4. Transfer via Mooncake (5 buffers):
   - embeddings, fill_ids, mrope_positions, aux_datas, deepstack_embeddings
```

### Language Side (æ¥æ”¶å’Œä½¿ç”¨)
```
1. Receive from Mooncake (5 buffers)
2. Gather from blocks:
   â†’ embedding_data (seq_len, hidden_size)
   â†’ deepstack_data (seq_len, hidden_size * 3)
3. Store to req:
   â†’ req.input_embeds = embedding_data
   â†’ req.input_deepstack_embeds = deepstack_data
4. Model forward:
   â†’ Qwen3MoeModel.forward(
       input_embeds=embedding_data,
       input_deepstack_embeds=deepstack_data  # â† è‡ªåŠ¨æ·»åŠ åˆ°å‰3å±‚
     )
```

## ğŸ”‘ å…³é”®è®¾è®¡

### 1. DeepStack ä¼ è¾“ç­–ç•¥
```
åˆå§‹ä¼ è¾“: embeddings + deepstack (å®Œæ•´æ•°æ®)
æ–­ç‚¹ç»­ä¼ : embeddings only (deepstackå·²ç¼“å­˜)

åŸå› :
- DeepStack åªç”¨äºå‰3å±‚ï¼Œä¸€æ¬¡ä¼ è¾“å³å¯
- èŠ‚çœå¸¦å®½ï¼Œé¿å…é‡å¤ä¼ è¾“
- Languageä¾§ç¼“å­˜ partial_deepstack_embeds ä¾›ç»­ä¼ ä½¿ç”¨
```

### 2. Buffer å¸ƒå±€
```
Block Structure (per block):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [0] input_embeddings (block_size * 8192)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [1] fill_ids (block_size)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [2] mrope_positions (3 * block_size)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [3] aux_datas (16)                          â”‚ â† Only first block, initial transfer
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [4] deepstack_embeddings                    â”‚ â† Only first block, initial transfer
â”‚     (block_size * 8192 * 3)                 â”‚    (3 layers worth)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. å…¼å®¹æ€§å¤„ç†
```python
# æ£€æŸ¥æ˜¯å¦æ”¯æŒ deepstack
if hasattr(model, "use_deepstack") and model.use_deepstack:
    # æå– deepstack
    ...
else:
    # ä¸æ”¯æŒåˆ™è®¾ä¸º None (å‘åå…¼å®¹)
    req.deepstack_embedding = None

# Buffer åˆå§‹åŒ–
MultimodalDataBuffers(
    size=...,
    block_size=...,
    embedding_dim=8192,
    num_deepstack_embeddings=3 if use_deepstack else 0  # 0 è¡¨ç¤ºç¦ç”¨
)
```

## âœ… éªŒè¯æ¸…å•

- [x] Phase 0: æ¨¡å‹å±‚é‡æ„å®Œæˆ
- [x] Phase 1: Buffer ç»“æ„æ‰©å±•å®Œæˆ
- [x] Phase 2: Encode ä¾§å®ç°å®Œæˆ
- [x] Phase 3: Language ä¾§å®ç°å®Œæˆ
- [x] Phase 4: ä¼ è¾“åè®®æ›´æ–°å®Œæˆ
- [x] Phase 5: ä»£ç éªŒè¯å®Œæˆ
- [x] æ—  linter errors
- [x] å®Œå…¨å‘åå…¼å®¹
- [x] æ”¯æŒæ–­ç‚¹ç»­ä¼ 

## ğŸ§ª æµ‹è¯•å»ºè®®

### 1. å•å…ƒæµ‹è¯•
```python
# Test 1: Buffer allocation/deallocation with deepstack
def test_multimodal_buffer_with_deepstack():
    buffer = MultimodalDataBuffers(
        size=10, 
        block_size=1024, 
        embedding_dim=8192,
        num_deepstack_embeddings=3
    )
    assert buffer.deepstack_embeddings.shape == (10, 1024 * 8192 * 3)

# Test 2: Scatter/Gather operations
def test_deepstack_scatter_gather():
    # Set buffer
    req.embedding = torch.randn(100, 8192)
    req.deepstack_embedding = torch.randn(100, 8192 * 3)
    buffer.set_buf(req)
    
    # Get buffer
    embed, fill_ids, mrope, aux, deepstack = buffer.get_buf(block_indices=[0])
    assert torch.allclose(embed, req.embedding)
    assert torch.allclose(deepstack, req.deepstack_embedding)

# Test 3: Backward compatibility (no deepstack)
def test_backward_compatibility():
    buffer = MultimodalDataBuffers(
        size=10, 
        block_size=1024,
        num_deepstack_embeddings=0  # Disabled
    )
    assert buffer.deepstack_embeddings is None
    embed, fill_ids, mrope, aux, deepstack = buffer.get_buf([0])
    assert deepstack is None
```

### 2. é›†æˆæµ‹è¯•
```python
# Test 1: End-to-end disaggregation
async def test_e2e_disaggregation():
    # Encode side
    encode_scheduler = create_encode_scheduler(model="qwen3-moe-vl")
    result = await encode_scheduler.forward(batch)
    
    # Should have deepstack
    assert hasattr(result.reqs[0], "deepstack_embedding")
    
    # Language side
    language_scheduler = create_language_scheduler(model="qwen3-moe")
    received = await language_scheduler.receive_embeddings()
    
    # Should receive deepstack
    assert hasattr(received.reqs[0], "input_deepstack_embeds")
    
    # Forward should use deepstack
    output = await language_scheduler.forward(received)
    # Verify output matches non-disaggregated mode

# Test 2: Resume transfer (deepstack cached)
async def test_resume_transfer():
    # Initial transfer (partial)
    initial_data = receive_partial_transfer()
    assert initial_data.partial_deepstack_embeds is not None
    
    # Resume transfer (no deepstack in transmission)
    resumed_data = receive_resumed_transfer()
    # Should use cached deepstack
    assert resumed_data.input_deepstack_embeds is not None
```

### 3. æ€§èƒ½æµ‹è¯•
```python
# Test memory usage
def test_memory_usage():
    # With deepstack: ~3x memory for embeddings
    # Without deepstack: baseline
    ...

# Test transfer speed
def test_transfer_speed():
    # Deepstack adds ~3x data to first block only
    # Should not significantly impact overall transfer time
    ...
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åˆå§‹åŒ– (Encode Side)
```python
# åˆ›å»º buffer (with deepstack)
metadata_buffers = MultimodalDataBuffers(
    size=1024,
    block_size=512,
    embedding_dim=8192,
    num_deepstack_embeddings=3,  # For Qwen3-VL-MoE
)

# Forward pass
result = model_runner.forward(batch)

# Extract deepstack
for req in batch.reqs:
    if model.use_deepstack:
        req.embedding, req.deepstack_embedding = (
            model.separate_deepstack_embeds(req.embedding)
        )
    
    # Store in buffer
    metadata_buffers.set_buf(req)
    
    # Transfer
    send_embedding_chunk(req)
```

### æ¥æ”¶ (Language Side)
```python
# Receive transfer
embedding_data, fill_ids, mrope, aux, deepstack = (
    metadata_buffers.get_buf(block_indices=[0, 1, 2])
)

# Store to request
req.input_embeds = embedding_data
req.input_deepstack_embeds = deepstack  # Will be used by model

# Forward (deepstack automatically used)
output = model.forward(
    input_ids=req.input_ids,
    positions=positions,
    forward_batch=batch,
    input_embeds=req.input_embeds,
    input_deepstack_embeds=req.input_deepstack_embeds,  # â† Added to layers 0-2
)
```

## ğŸ‰ å®ç°å®Œæˆ

æ‰€æœ‰ 5 ä¸ª Phase å·²å…¨éƒ¨å®æ–½å®Œæ¯•ï¼

- âœ… æ¨¡å‹å±‚æ”¯æŒ deepstack
- âœ… Buffer æ”¯æŒ deepstack å­˜å‚¨
- âœ… Encode ä¾§æå–å’Œä¼ è¾“ deepstack
- âœ… Language ä¾§æ¥æ”¶å’Œä½¿ç”¨ deepstack
- âœ… ä¼ è¾“åè®®å¤„ç† deepstack blocks
- âœ… æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- âœ… å®Œå…¨å‘åå…¼å®¹

**çŠ¶æ€**: ğŸŸ¢ å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥æµ‹è¯•å’Œéƒ¨ç½²

---

**å®Œæˆæ—¶é—´**: 2025-10-24  
**æ€»è€—æ—¶**: Phase 0-5 å…¨éƒ¨å®Œæˆ  
**ä»£ç è´¨é‡**: âœ… ä¼˜ç§€ (æ—  linter errors)  
**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæ•´
